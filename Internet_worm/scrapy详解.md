[TOC]



# scrapy:

- ä»€ä¹ˆæ˜¯æ¡†æ¶?
  - ç»§æ‰¿äº†å„ç§åŠŸèƒ½ä¸”å…·æœ‰å¾ˆå¼ºé€šç”¨æ€§(å¯ä»¥è¢«åº”ç”¨åœ¨å„ç§ä¸åŒçš„éœ€æ±‚ä¸­)çš„ä¸€ä¸ªé¡¹ç›®æ¨¡æ¿
  - æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯å­¦ä¹ æ€ä¹ˆä½¿ç”¨è¿™äº›æ¡†æ¶çš„åŠŸèƒ½

- scrapyæ¡†æ¶é›†æˆçš„åŠŸèƒ½æœ‰å“ªäº›?
  - é«˜æ€§èƒ½çš„æ•°æ®è§£ææ“ä½œ
  - é«˜æ€§èƒ½çš„æ•°æ®ä¸‹è½½æ“ä½œ
  - æŒä¹…åŒ–æ•°æ®å­˜å‚¨

- scrapyé€šå¸¸åªç”¨äºgetè¯·æ±‚,å¹¶ä¸é€‚ç”¨äºpostè¯·æ±‚çš„æ¨¡æ‹Ÿç™»é™†

- scrapyç¯å¢ƒå®‰è£…
  1. pip install wheel
  2. pip install twisted
     - [twistedä¸‹è½½](https://pypi.org/project/Twisted/#files)
  3. pip install pywin32
  4. pip install scrapy

- å¼€å§‹åˆ›å»ºscrapyå·¥ç¨‹
  - è¿›å…¥ç»ˆç«¯è¾“å…¥æŒ‡ä»¤åˆ›å»ºæ–°çš„scrapyå·¥ç¨‹
  - â€˜scrapy startproject projectnameâ€™
  - æŒ‰ç…§æŒ‡ä»¤åˆ›å»ºæ–°çš„çˆ¬è™«æ–‡ä»¶ 
  - scrapy genspider spiderName www.xxx.com
  - å¯åŠ¨çˆ¬è™«ç¨‹åº
    - scrapyçˆ¬è™«ä¸èƒ½ç›´æ¥è¿è¡Œ,
    - åœ¨å‘½ä»¤çª—å£ä¸­è¾“å…¥æŒ‡ä»¤
    - scrapy crawl spiderName
      - scrapy crawl spiderName --nolog
      - è¿è¡Œæ—¶ä¸è¾“å‡ºæ—¥å¿—ä¿¡æ¯
      - åœ¨settingæ–‡ä»¶ä¸­æ·»åŠ é…ç½®
      - LOG_LEVEL= â€˜ERROR'â€™
      - å½“å‘ç”Ÿé”™è¯¯æ—¶,å°†é”™è¯¯æ—¥å¿—è¾“å‡º,æ–¹ä¾¿è°ƒè¯•

- scrapyå·¥ç¨‹è®¾ç½®--â€”â€“-setting

  - User_Agent    è¿™é‡Œè®¾ç½®çš„æ˜¯çˆ¬è™«çš„è¯·æ±‚å¤´
  - ROBOTSTXT_OBEY = True   è¿™é‡Œè®¾ç½®çš„æ˜¯robotsåè®®,å½“å€¼ä¸ºTrueæ—¶,çˆ¬è™«å°†ä¼šåœ¨è¿è¡Œæ—¶ä¼˜å…ˆæŸ¥çœ‹robotsåè®®,å¦‚æœåè®®ä¸å…è®¸å°†ä¸ä¼šè¿›è¡Œçˆ¬å–

- scrapyå·¥ç¨‹ä½¿ç”¨

  - æ„å»ºè§£æ

  - ```python
    # -*- coding: utf-8 -*-
    import scrapy
    class NewSpiderSpider(scrapy.Spider):
        # çˆ¬è™«æ–‡ä»¶çš„åç§°,ç›¸å½“äºçˆ¬è™«æ–‡ä»¶çš„å”¯ä¸€æ ‡è¯†
        name = 'ğŸ’€'
        # å¾ªåºçš„åŸŸå, é€šå¸¸æƒ…å†µä¸‹ä¸ä¼šä½¿ç”¨
        # allowed_domains = ['www.baidu.com']
        # èµ·å§‹çš„urlåˆ—è¡¨, scrapyå°†ä¼šå¯¹åˆ—è¡¨ä¸­çš„urlè‡ªåŠ¨è¿›è¡Œè¯·æ±‚å‘é€
        start_urls = ['http://www.budejie.com/']
    
        def parse(self, response):
            # åœ¨scrapyä¸­,æ•°æ®è§£æä¸éœ€è¦æ‰‹åŠ¨å¯¼å…¥etreeæ¥è¿›è¡Œ,ç›¸å¯¹çš„,è¿™é‡Œé¢é›†æˆäº†etree çš„åŠŸèƒ½,ä¹Ÿå°±æ˜¯è¯´,æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨scrapyçš„é›†æˆæ¥è¾¾åˆ°æˆ‘ä»¬è¿›è¡Œæ•°æ®è§£æçš„ç›®æ ‡
            # è¿™ä¸ªåŠŸèƒ½çš„ä½¿ç”¨æ–¹å¼å¦‚ä¸‹
            res = response.xpath('//div[@class="j-r-list-c-desc"]/a/text()').extract()
            # extractæ–¹æ³•,æå–è·å–åˆ°çš„selectedå¯¹è±¡ä¸­çš„dataæ•°æ®,å½“å¯¹è±¡ä¸ºå•ä¸ªå¯¹è±¡çš„æ—¶å€™,è·å–åˆ°çš„å¯¹è±¡å°±æ˜¯å•ä¸ªçš„å­—ç¬¦ä¸²,
            # å½“æå–åˆ°çš„selectedå¯¹è±¡ä¸ºlistå¯¹è±¡æ—¶,è·å–åˆ°çš„æ•°æ®ä¹Ÿä¼šè‡ªåŠ¨å˜æˆä¸€ä¸ªåˆ—è¡¨,å¹¶ä¸éœ€è¦å¾ªç¯éå†selectedåˆ—è¡¨æ¥è¿›è¡Œæå–
            for i in res:
                print(i)
    ```

- scrapyæŒä¹…åŒ–å­˜å‚¨

  - åŸºäºç»ˆç«¯çª—å£çš„æŒä¹…åŒ–å­˜å‚¨

    - ç‰¹æ€§:åªå¯ä»¥å°†parseæ–¹æ³•çš„è¿”å›å€¼å†™å…¥åˆ°æœ¬åœ°çš„ç£ç›˜æ–‡ä»¶ä¸­

    - æŒ‡ä»¤: scrapy crawl spiderName -o filePath

    - ```python
          def parse(self, response):
              li_list = response.xpath('//div[@class="j-r-list"]/ul/li')
              all_data = []
              for li in li_list:
                  author = li.xpath('./div[1]/div[2]/a/text()').extract()[0]
                  content = li.xpath('./div[2]/div[1]/a/text()').extract()[0]
                  dic = {
                      'Author': author,
                      'Content': content
                  }
                  all_data.append(dic)
              return all_data
      # scrapy å°†ä¼šå¯¹parseçš„è¿”å›å€¼è¿›è¡Œå¤„ç†,å¹¶é¢„ç½®äº†å†…éƒ¨çš„æŒä¹…åŒ–å­˜å‚¨æ¨¡å—.è¿™ä¸ªæŒä¹…åŒ–å­˜å‚¨åŸºäºå‘½ä»¤ç»ˆç«¯è¿è¡Œ,è¯¥é€‰é¡¹ä¸æ”¯æŒtxtæ–‡ä»¶å­˜å‚¨,ç›®å‰ä»…æ”¯æŒ('json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle')è¿™ä¸ƒç§æ–‡ä»¶
      ```

    - 

  - åŸºäºç®¡é“çš„æŒä¹…åŒ–å­˜å‚¨

    - ç®¡é“è¯å­˜å‚¨éœ€è¦ä½¿ç”¨scrapyä¸­å°å­˜çš„ä¸€äº›æ–¹æ³•,åŒæ—¶éœ€è¦è¿›è¡Œä¸€äº›å¤„ç†,è¿™æ ·çš„å­˜å‚¨æ–¹å¼æ”¯æŒtxtæ–‡æ¡£å­˜å‚¨

    - ç®¡é“æ–‡ä»¶å­˜å‚¨ç¤ºä¾‹:

    - ```python
      class FirstblodPipeline(object):
          fp = None
          # è®¾å®šåœ¨ç®¡é“è¿è¡Œå¼€å§‹ä¹‹å‰ä¼˜å…ˆè¿è¡Œçš„ä»£ç 
          def open_spider(self, spider):
              print('çˆ¬è™«å¼€å§‹è¿è¡Œ~~~~~~~')
              # é€šè¿‡åœ¨ç®¡é“è¿è¡Œå¼€å§‹ä¹‹å‰æ‰“å¼€æ–‡ä»¶çš„å½¢å¼æ¥ç¡®ä¿è¿™ä¸ªæ–‡ä»¶æ¯æ¬¡åªéœ€è¦æ‰“å¼€ä¸€æ¬¡
              self.fp = open('./firstBlod/spiders/all_data.txt', 'w', encoding='utf-8')
      
          # å½“æ•°æ®è¢«æäº¤æ—¶æ‰§è¡Œçš„ä»£ç ,åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®æŒä¹…åŒ–å­˜å‚¨
          def process_item(self, item, spider):
              author = item['author']
              content = item['content']
              # è°ƒç”¨å·²ç»æ‰“å¼€çš„æ–‡ä»¶,å¹¶åœ¨é‡Œé¢è¿›è¡Œå†™å…¥æ“ä½œ
              self.fp.write(author + ':' + content + '\n')
              # å°†itemè½¬äº¤ç»™ä¸‹ä¸€ä¸ªç®¡é“ç±»
              return item
          # å½“ç®¡é“å…³é—­æ—¶æ‰§è¡Œçš„ä»£ç 
          def close_spider(self, spider):
              print('çˆ¬è™«ç»“æŸè¿è¡Œ~~~~~~~')
              # åœ¨ä»£ç æ•´ä½“è¿è¡Œç»“æŸçš„æ—¶å€™,å…³é—­æ‰“å¼€çš„æ–‡ä»¶
              self.fp.close()
      
      ```

    - itemæ–‡ä»¶å¯¹è±¡å®ä¾‹

    - ```python
      class FirstblodItem(scrapy.Item):
          # define the fields for your item here like:
          # name = scrapy.Field()
          # æ„å»ºæ–°çš„itemå¯¹è±¡ä¸­çš„å‚æ•°
          # æ ¼å¼ä¸º    name = scrapy.Field()
          # ä½¿ç”¨fieldæ–‡ä»¶æ ¼å¼çš„æ—¶å€™,å…¼å®¹å‡ ä¹æ‰€æœ‰çš„æ–‡ä»¶ç±»å‹
          # åŒ…æ‹¬ä½†ä¸ä»…é™äº   åˆ—è¡¨,å…ƒç»„,å­—å…¸,å­—ç¬¦ä¸²,æ•°å­—,äºŒè¿›åˆ¶æµç­‰å„ç§å„æ ·çš„å½¢å¼
          author = scrapy.Field()
          content = scrapy.Field()
      ```

    - settingsæ–‡ä»¶å®ä¾‹

    - ```python
      # Configure item pipelines
      # See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
      ITEM_PIPELINES = {
         'firstBlod.pipelines.FirstblodPipeline': 300,
          # ç®¡é“å¯¹è±¡çš„åå­—,åé¢çš„æ•°å€¼ä¸ºç®¡é“ä¼˜å…ˆå€¼
          # ä¼˜å…ˆå€¼é«˜çš„å°†ä¼šä¼˜å…ˆæ‰§è¡Œ
          # æ•°å€¼è¶Šä½ä¼˜å…ˆå€¼è¶Šé«˜
      }
      ```

    - æ‰¾åˆ°ç®¡é“ç›¸å…³ä»£ç è¿›è¡Œæ¿€æ´»

    - åœ¨çˆ¬è™«æ–‡ä»¶ä¸­è¿›è¡Œä½¿ç”¨

    - ```python
      import scrapy
      from firstBlod.items import FirstblodItem
      # å¯¼å…¥itemè®¾å®šç±»
      class NewSpiderSpider(scrapy.Spider):
          name = 'ğŸ’€'
          start_urls = ['http://www.budejie.com/']
      
          def parse(self, response):
              li_list = response.xpath('//div[@class="j-r-list"]/ul/li')
              # all_data = []
              for li in li_list:
                  author = li.xpath('./div[1]/div[2]/a/text()').extract()[0]
                  content = li.xpath('./div[2]/div[1]/a/text()').extract()[0]
                  item = FirstblodItem()
                  # å®ä¾‹åŒ–itemå¯¹è±¡
                  item['author'] = author
                  item['content'] = content
                  # itemä½¿ç”¨æ–¹æ³•å’Œå­—å…¸ç±»ä¼¼
                  # ä½¿ç”¨yieldæ–¹æ³•è¿”å›å°è£…å®Œæˆçš„itemå¯¹è±¡
                  # å½“ä½¿ç”¨yieldè¿”å›çš„æ—¶å€™,å°†ä¼šè‡ªåŠ¨å°†itemä¼ è¾“åˆ°ç®¡é“å¯¹åº”çš„æ¥å—ç±»ä¸­,è¿›è¡Œå¤„ç†å¹¶æŒä¹…åŒ–å­˜å‚¨
                  yield item
      ```

    - å°†ä¸Šè¿°æ¡ä»¶å‡†å¤‡å®Œæ¯•ä¹‹å,å°±å¯ä»¥è¿›è¡Œæ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨äº†

    - å½“ç„¶,ä¹Ÿä¸ä¸€å®šéè¦å­˜å‚¨åˆ°æ–‡ä»¶ä¸­å»,æ¯•ç«Ÿè¿˜æœ‰ä¸€äº›æ¶‰åŠåˆ°æ•°æ®åº“çš„å­˜å‚¨,éƒ½å¯ä»¥æ”¾åˆ°è¿™ä¸ªç®¡é“ä¸­æ¥è¿›è¡Œ

    - ä¹Ÿå°±æ˜¯,ç®¡é“æ–‡ä»¶ä¸­çš„ä¸€ä¸ªç®¡é“ç±»è´Ÿè´£ä¸€ç§æŒä¹…åŒ–å­˜å‚¨çš„æ–¹æ¡ˆ

    - itemæäº¤çš„æ—¶å€™å°†ä¼šæŠŠitemäº¤ç»™ä¼˜å…ˆçº§æœ€é«˜çš„ç®¡é“ç±»

    - åœ¨ç®¡é“ç±»ä¸­,return itemçš„ä½œç”¨æ˜¯å°†itemè½¬äº¤ç»™ä¸‹ä¸€ä¸ªç®¡é“ç±»

    - åŒç®¡é“ç±»å†™æ³•ä»¥åŠMysqlæ•°æ®åº“å†™å…¥

    - ```python
      class MysqlPip(object):
          conn = None
          cursor = None
      
          # è®¾å®šåœ¨ç®¡é“è¿è¡Œå¼€å§‹ä¹‹å‰ä¼˜å…ˆè¿è¡Œçš„ä»£ç 
          def open_spider(self, spider):
              # é€šè¿‡åœ¨ç®¡é“è¿è¡Œå¼€å§‹ä¹‹å‰æ‰“å¼€æ–‡ä»¶çš„å½¢å¼æ¥ç¡®ä¿è¿™ä¸ªæ–‡ä»¶æ¯æ¬¡åªéœ€è¦æ‰“å¼€ä¸€æ¬¡
              print('çˆ¬è™«2å¼€å§‹è¿è¡Œ~~~~~~~')
              # å»ºç«‹æ•°æ®åº“æ¸¸æ ‡
              self.conn = pymysql.Connect(host='127.0.0.1',port=3306,user='root',password='000000',db='spider',charset='utf8')
              print(self.conn)
          # å½“æ•°æ®è¢«æäº¤æ—¶æ‰§è¡Œçš„ä»£ç ,åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®æŒä¹…åŒ–å­˜å‚¨
          def process_item(self, item, spider):
              author = item['author']
              content = item['content']
              sql = 'insert into bs values ("%s","%s")'% (author,content)
              print(sql)
              self.cursor = self.conn.cursor()
              try:
                  self.cursor.execute(sql)
                  self.conn.commit()
              except Exception as e:
                  print(e)
                  self.conn.rollback()
              print('----------------------------------------------------------------------------')
              return item
          def close_spider(self, spider):
              print('çˆ¬è™«2ç»“æŸè¿è¡Œ~~~~~~~')
              self.cursor.close()
              self.conn.close()
      ```

  - æ‰‹åŠ¨å‘é€è¯·æ±‚:

    - æ‰‹åŠ¨å‘é€getè¯·æ±‚

      - åº”ç”¨åœºæ™¯:   è¯·æ±‚åŒä¸€ä¸ªä¸»é¡µé¢ä¸‹çš„å¤šä¸ªé¡µé¢

      - ä»£ç 

        ```python
        # -*- coding: utf-8 -*-
        import scrapy
        from firstBlod.items import FirstblodItem
        
        class NewSpiderSpider(scrapy.Spider):
            # çˆ¬è™«æ–‡ä»¶çš„åç§°,ç›¸å½“äºçˆ¬è™«æ–‡ä»¶çš„å”¯ä¸€æ ‡è¯†
            name = 'new_spider'
            # å¾ªåºçš„åŸŸå, é€šå¸¸æƒ…å†µä¸‹ä¸ä¼šä½¿ç”¨
            # allowed_domains = ['www.baidu.com']
            # èµ·å§‹çš„urlåˆ—è¡¨, scrapyå°†ä¼šå¯¹åˆ—è¡¨ä¸­çš„urlè‡ªåŠ¨è¿›è¡Œè¯·æ±‚å‘é€
            start_urls = [
                'https://www.zhipin.com/job_detail/?query=python&city=101010100&industry=&position=',
                ]
            # è®¾ç½®åŸºæœ¬çš„urlæ¨¡æ¿
            url = 'https://www.zhipin.com/c101010100/?query=python&page=%d&ka=page-%d'
            # è®°å½•é¡µæ•°
            page_name = 1
        
            def parse(self, response):
                li_list = response.xpath('//div[@class="job-list"]')
                print(li_list)
                # all_data = []
                for li in li_list:
                    # èŒä½åç§°
                    Job_title = li.xpath('./div/div[1]/h3/a/div[1]/text()').extract()[0]
                    # å…¬å¸åç§°
                    Corporate_name = li.xpath('./div/div[1]/a/text()').extract()[0]
                    # åœ°å€
                    all_address = li.xpath('./div/div[1]/p//text()').extract()[0]
                    # ç»éªŒ experience
                    # address,experience,Education = all_address.split('|')
                    #å­¦å† Education
                    # dic = {
                    #     'Author': author,
                    #     'Content': content
                    # }
                    # all_data.append(dic)
                    item = FirstblodItem()
                    item['Job_title'] = Job_title
                    item['Corporate_name'] = Corporate_name
                    yield item
                # ç¡®å®šå½“å‰é¡µæ•°,
                if self.page_name <= 5:
                    self.page_name += 1
                    # æ‹¼æ¥æ–°çš„url
                    new_url = format(self.url%(self.page_name,self.page_name))
                    # ä½¿ç”¨yieldè¿›è¡Œè¯·æ±‚,å‚æ•°callbackä¸ºå¤„ç†è¿™äº›é¡µé¢ä¿¡æ¯æ‰€ç”¨çš„å‡½æ•°,å¯è‡ªè¡Œè®¾å®š,ä¹Ÿå¯ç”¨é€’å½’çš„æ–¹å¼ä½¿ç”¨å½“å‰çš„å‡½æ•°
                    yield scrapy.Request(new_url,callback=self.parse)
                else:
                    return li_list
        ```

      - é‡å†™çˆ¶ç±»çš„è‡ªåŠ¨è¯·æ±‚æ–¹æ³•

        ```python
        # é‡å†™çˆ¶ç±»æ–¹æ³•æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰æ•°æ®æ¸…æ´—å‡½æ•°,è€Œä¸éœ€è¦å±€é™äºparse
        def start_requests(self):
        	for url in self.start_urls:
        		yield scrapy.Request(url,callback=self.parse)
        ```

      - è¦æƒ³ä½¿scrapyè‡ªåŠ¨å‘é€getè¯·æ±‚,éœ€è¦é‡å†™start_requestsæ–¹æ³•

    

    ## scrapyå›¾ç‰‡å¤„ç†:

  1. åœ¨scrapyä¸­,æœ‰ç€ä¸“é—¨çš„æ¨¡å—å¯¹å›¾ç‰‡æ•°æ®è¿›è¡Œè¯·æ±‚ä»¥åŠå¤„ç†,æˆ‘ä»¬åªéœ€è¦å°†è·å–åˆ°çš„å›¾ç‰‡urlä»¥itemçš„å½¢å¼ä¼ è¾“åˆ°æˆ‘ä»¬çš„ç®¡é“ä¹‹ä¸­è¿›è¡Œå¤„ç†å³å¯,itemå¯¹è±¡çš„åˆ›å»ºäºå¯»å¸¸çš„åˆ›å»ºæ–¹æ³•æ²¡è®¾ä¹ˆåŒºåˆ«,scrapy.FieldsåŒ…å®¹æ€§æå¼º,ä¸éœ€è¦è€ƒè™‘å…¼å®¹æ€§é—®é¢˜,

  2. åœ¨æäº¤åˆ°ç®¡é“ä¸­å»ä¹‹å,æˆ‘ä»¬å¯ä»¥åœ¨ç®¡é“ä¹‹ä¸­å¯¹itemä¸­çš„æ•°æ®è¿›è¡Œå¤„ç†,

  3. ä½¿ç”¨scrapyä¸­å°è£…çš„å›¾ç‰‡å¤„ç†ä¸“ç”¨ç±»scrapy.pipelines.images import ImagePipeline

  4. åˆ›å»ºä¸€ä¸ªæ–°çš„ç®¡é“ç±»,è¿™ä¸ªç®¡é“ç±»ç»§æ‰¿è‡ªImagePipline

  5. ```python
     class ImgSpidersPipeline(ImagesPipeline):
     
         # å¯¹ä¸€ä¸ªå›¾ç‰‡é“¾æ¥è¿›è¡Œè¯·æ±‚å‘é€
         # item å°±æ˜¯scrapyæäº¤è¿‡æ¥çš„itemæ•°æ®
         def get_media_requests(self, item, info):
             yield scrapy.Request(item['src'])
     
         # å‡†å¤‡æ–‡ä»¶å
         def file_path(self, request, response=None, info=None):
             file_name = request.url.split('/')[-1]
             print('æ­£åœ¨ä¸‹è½½',file_name,'............')
             return file_name
         # å°†itemä¼ é€’ç»™ä¸‹ä¸€ä¸ªå³å°†æ‰§è¡Œçš„ç®¡é“ç±»
         def item_completed(self, results, item, info):
             return item
     ```

  6. å®Œæˆè¿™äº›ä¹‹å,æˆ‘ä»¬è¿˜éœ€è¦å¯¹å›¾ç‰‡çš„å­˜å‚¨è·¯å¾„è¿›è¡Œè®¾ç½®,åœ¨settingæ–‡ä»¶ä¸­æ·»åŠ æ–°çš„å±æ€§

     - IMAGES_STORE = â€œimagePathâ€

     - imagePathè‡ªè¡Œè®¾ç½®,æœ€å¥½è®¾ç½®ä¸ºç»å¯¹è·¯å¾„,å¦‚æœä½¿ç”¨ç›¸å¯¹è·¯å¾„çš„è¯,æœ€å¥½å‚è€ƒdjangoä¸­çš„æ–‡ä»¶è·¯å¾„çš„å†™æ³•,å…ˆç¡®å®šå·¥ç¨‹çš„è·¯å¾„,ç„¶åå†åœ¨å·¥ç¨‹è·¯å¾„çš„åŸºç¡€ä¸Šè®¾å®šç›¸å¯¹è·¯å¾„

     - djangoæ–¹æ³•è®¾ç½®è·¯å¾„å¦‚ä¸‹:

     - ```python
       import os
       # è·å–å½“å‰æ–‡ä»¶å¤¹çš„ä¸»è·¯å¾„
       BASE_DIR = os.path.dirname(os.path.abspath(__file__))
       # å°†è¦æ·»åŠ çš„å›¾ç‰‡æ–‡ä»¶ä¿å­˜è·¯å¾„åŠ å…¥åˆ°ä¸»è·¯å¾„ä¸­å»
       IMAGES_STORE = os.path.join(BASE_DIR,'imgsLib')
       ```

     - è¿™ä¸ªæ–¹æ³•åŒæ ·è®¾ç½®åœ¨settingsæ–‡ä»¶ä¸­

  7. å®Œæˆè¿™äº›ä¹‹å,åªéœ€è¦åœ¨settingsä¸­å°†å†™å¥½çš„ç®¡é“ç±»è¿›è¡Œæ³¨å†Œ,å³å¯å¼€å§‹è¿™ä¸ªç®¡é“ç±»

  8. è¿è¡Œçˆ¬è™«ç¨‹åº,å°†ä¼šå¼€å§‹å…¨è‡ªåŠ¨ä¸‹è½½é€‰ä¸­çš„å›¾ç‰‡,æ ¹æ®ä¹‹å‰å­¦è¿‡çš„æ–¹æ³•,å³è®¾å®šç½‘å€çš„åŸºç¡€æ¨¡æ¿,å³å¯å®ç°å›¾ç‰‡çš„æ‰¹é‡ä¸‹è½½



## å¦‚ä½•æé«˜scrapyçˆ¬è™«çš„æ•ˆç‡

  1. å¢åŠ å¹¶å‘æ•°é‡
     - CONCURRENT_REQUESTS = 32
     - é»˜è®¤å¹¶å‘æ•°é‡ä¸º32
     - å¯ä»¥è‡ªè¡Œè®¾ç½®
  2. é™ä½æ—¥å¿—ç­‰çº§
     - LOG_LEVEL = "INFO"
     - LOG_LEVEL = "ERROR"
  3. ç¦æ­¢cookie
     - COOKIES_ENABLED = False
     - åœ¨scrapyä¸­ä¼šè‡ªåŠ¨å¯¹cookieè¿›è¡Œå¤„ç†,ä¸ç®¡è¿™ä¸ªé¡µé¢æ˜¯å¦éœ€è¦éªŒè¯cookie
     - å°†cookieå¤„ç†æ¨¡å—è¿›è¡Œå…³é—­,å°†ä¼šæå‡scrapyçš„æ‰§è¡Œæ•ˆç‡
  4. ç¦æ­¢é‡è¯•
     - scrapyå°†ä¼šè‡ªåŠ¨å¯¹å¤±è´¥çš„è¯·æ±‚è¿›è¡Œé‡è¯•,è¿™ä¸¥é‡å½±å“åˆ°äº†çˆ¬è™«çš„æ‰§è¡Œæ•ˆç‡,å¯ä»¥å°†é‡è¯•åŠŸèƒ½ç¦æ‰,ä»è€Œæå‡æ‰§è¡Œæ•ˆç‡
     - åœ¨settingsæ–‡ä»¶ä¸­ä¹¦å†™ä»£ç 
     - RETRY_ENABLED = False
     - å½“è¯·æ±‚å¤±è´¥çš„æ—¶å€™,scrapyå°†ä¸ä¼šå»å¤„ç†è¯·æ±‚å¤±è´¥çš„æ•°æ®,è€Œæ˜¯ä¼šç›´æ¥è·³è¿‡è¿›è¡Œä¸‹ä¸€æ¡
  5. å‡å°‘ä¸‹è½½è¶…æ—¶
     - DOWNLOAD_TIMEOUT = 10
     - å†™å…¥è¿™æ®µä»£ç ,ä½œç”¨æ˜¯è®¾å®šè¯·æ±‚è¶…æ—¶çš„æ—¶é—´,ä¹Ÿå°±æ˜¯è¯´,å½“ä½ åœ¨äº²æ±‚æ—¶é—´è¶…è¿‡äº†åç§’çš„æ—¶å€™ä¾ç„¶æ²¡æœ‰æ‹¿åˆ°æ•°æ®çš„è¯,å°†ä¼šç»“æŸè¯·æ±‚,æ‰§è¡Œä¸‹ä¸€é¡¹,è€Œä¸æ˜¯ä¸€ç›´ç­‰å¾…ä¸‹å»,è¿™ä¸ªç†ŸçŸ¥çš„å•ä½æ˜¯ç§’
## è¯·æ±‚ä¼ å‚:

- åŸºäºè¯·æ±‚ä¼ å‚å¯ä»¥å®ç°æ·±åº¦çˆ¬å–
  - è¯·æ±‚ä¼ å‚,åœ¨è¿›è¡Œscrapy.Requestè¯·æ±‚çš„æ—¶å€™,å¯ä»¥ä½¿ç”¨ç¬¬ä¸‰ä¸ªå‚æ•°meta
  - è¿™ä¸ªå‚æ•°çš„ä½œç”¨æ˜¯å‘ä¸Šä¸€ä¸ªå‚æ•°callbackè¿™ä¸ªè§£æå‡½æ•°ä¸­ä¼ é€’å‚æ•°,å¥¹çš„æ•°æ®ç±»å‹æ˜¯ä¸€ä¸ªå­—å…¸,é€šè¿‡é”®å€¼å¯¹çš„å½¢å¼å­˜å‚¨éœ€è¦ä¼ é€’çš„æ•°æ®
  - å½“æˆ‘ä»¬éœ€è¦å°†ä¸Šä¸€ä¸ªè§£æå‡½æ•°ä¸­å®ä¾‹åŒ–å¥½çš„itemå¯¹è±¡ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªè§£æå‡½æ•°ä¸­çš„æ—¶å€™,å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•è¿›è¡Œä¼ é€’
  - åœ¨å¦ä¸€ä¸ªè§£æå‡½æ•° ä¸­,é€šè¿‡responseå‚æ•°çš„metaå±æ€§å¯ä»¥æ‹¿åˆ°å‚æ•°çš„å†…å®¹å¹¶è¿›è¡Œæå–,è¿™ä¸ªå°±å¯ä»¥æ‹¿åˆ°ä¸Šä¸€ä¸ªå‡½æ•°ä¸­ä¼ é€’è¿‡æ¥çš„itemå¯¹è±¡,å®ç°ä¸åŒè§£æå‡½æ•°ä¹‹é—´çš„è¯·æ±‚ä¼ é€’
  - é€šè¿‡è¿™ä¸ªæ–¹å¼å¯ä»¥å®ç°æ·±åº¦çˆ¬å–,å³
  - å½“ä¸€æ•´å¥—æ•°æ®çš„å†…å®¹å¦‚æ ‡é¢˜å’Œç®€ä»‹å­˜åœ¨äºä¸¤ä¸ªå…³è”çš„é¡µé¢ä¸­,é‚£ä¹ˆæˆ‘ä»¬æƒ³è¦åŒæ—¶è·å¾—æ ‡é¢˜å’Œç®€ä»‹,å°±éœ€è¦è¿›è¡Œæ·±åº¦çˆ¬å–,ä½¿ç”¨è¿™ä¸ªæ–¹å¼ä¼ é€’çš„è¯,å°†ä¼šæ›´ç®€ä¾¿çš„å®ç°æ·±åº¦çˆ¬å–çš„è¦æ±‚





## ä¸­é—´ä»¶:

åœ¨åˆ›å»ºå¥½çš„scrapyå·¥ç¨‹ä¸­,è‡ªå¸¦äº†ä¸¤ä¸ªåŸºç¡€çš„ä¸­é—´ä»¶

çˆ¬è™«ä¸­é—´ä»¶TwoSpidersSpiderMiddleware

ä¸‹è½½ä¸­é—´ä»¶TwoSpidersDownloaderMiddleware

1. ### æ‹¦æˆªä¸­é—´ä»¶

   1. ä½œç”¨:æ‰¹é‡æ‹¦æˆªè¯·æ±‚

   2. æ‹¦æˆªè¯·æ±‚:
      - UAä¼ªè£…

        - ç›®çš„:å°†æ‰€æœ‰çš„è¯·æ±‚çš„è¯·æ±‚å¤´å°½å¯èƒ½å¤šçš„ä¸åŒçš„è¯·æ±‚è½½ä½“æ ‡è¯†

      - ä»£ç†æ“ä½œ

        - ```python
              def process_exception(self, request, exception, spider):
                  # åœ¨é”™è¯¯æ•è·é˜¶æ®µè¿›è¡Œä»£ç†ä¿®æ­£,
                  if request.url.split(':')[0] == 'http':
                      request.meta['proxy'] = 'http://' + random.choice(PROXY_http)
                  else:
                      request.meta['proxy'] = 'http://' + random.choice(PROXY_https)
                  return request
          ```

        - ```python
              def process_request(self, request, spider):
                  # å®ç°å°†æ‹¦æˆªåˆ°çš„requestè¯·æ±‚å°½å¯èƒ½å¤šçš„è®¾å®šæˆä¸åŒçš„è¯·æ±‚è½½ä½“èº«ä»½æ ‡è¯†
                  request.headers['User-Agent'] = random.choice(user_agent_list)
                  # åœ¨æ¯æ¬¡è¯·æ±‚ä¹‹å‰è¿›è¡Œä»£ç†ä¿®æ­£
                  if request.url.split(':')[0] == 'http':
                      request.meta['proxy'] = 'http://' + random.choice(PROXY_http)
                  else:
                      request.meta['proxy'] = 'http://' + random.choice(PROXY_https)
                  return None
          ```

   3. æ‹¦æˆªå“åº”

      - ç¯¡æ”¹å“åº”å¯¹è±¡æˆ–ç›´æ¥æ›¿æ¢å“åº”å¯¹è±¡

2. ä¸‹è½½ä¸­é—´ä»¶ 

   1. ```python
       import scrapy
       from selenium.webdriver import Chrome,ChromeOptions
       from selenium.webdriver.chrome.options import Options
       from wangyi.items import WangyiItem
       # åˆ›å»ºoptionså®ä¾‹å¯¹è±¡,å®ç°æ— å¤´æµè§ˆå™¨
       chrome_option = Options()
       chrome_option.add_argument('--headless')
       chrome_option.add_argument('--disable-gpu')
       class WangyiSpider(scrapy.Spider):
           name = 'wangyi'
           # allowed_domains = ['www.xxx.com']
           # åŸå§‹urlç½‘é¡µ
           start_urls = ['https://news.163.com/']
           pro = Chrome(chrome_options=chrome_option)
           # ç”¨æ¥å­˜å‚¨åç»­çš„æ‰€æœ‰å­é¡µé¢çš„url
           cls_url_list = []
       
           def parse(self, response):
               # å‡†å¤‡è¦çˆ¬å–çš„ç›®æ ‡
               index_list = [3,4]
               li_list = response.xpath('//*[@id="index2016_wrap"]/div[1]/div[2]/div[2]/div[2]/div[2]/div/ul/li')
               # print(li_list)
               for index in index_list:
                   li = li_list[index]
                   # è·å–å­é¡µé¢url
                   new_url = li.xpath('./a/@href').extract_first()
                   # è·å–ç‹¬å½±çš„åˆ†ç±»åç§°
                   news = li.xpath('./a/text()').extract_first()
                   # å°†å­é¡µé¢çš„urlæ·»åŠ åˆ°ç±»å±æ€§ä¸­è¿›è¡Œå­˜å‚¨
                   self.cls_url_list.append(new_url)
                   # å¼€å§‹å¯¹å­é¡µé¢è¿›è¡Œè¿‡è¯·æ±‚,å¹¶å°†è¯·æ±‚æ•°æ®äº¤ç»™ä¸‹ä¸€ä¸ªè§£æå‡½æ•°
                   yield scrapy.Request(url=new_url,callback=self.new_prase)
           
           def new_prase(self,response):
               div_list = response.xpath('/html/body/div/div[3]/div[4]/div[1]/div/div/ul/li/div/div')
               for div in div_list:
                   # è·å¾—æ–°é—»æ ‡é¢˜ä¸æ–°é—»è¯¦æƒ…é¡µé¢çš„url
                   title = div.xpath('./div/div/h3/a/text()').extract_first()
                   new_url = div.xpath('./div/div/h3/a/@href').extract_first()
                   # åˆ›å»ºitemå¯¹è±¡,å¹¶å°†åé¢ä¼šç”¨åˆ°çš„æ ‡é¢˜å­˜å‚¨è¿›å»
                   item = WangyiItem()
                   item['title'] = title
                   # å¼€å§‹å¯¹è¯¦æƒ…é¡µé¢è¿›è¡Œè¯·æ±‚,å¹¶å°†è·å–åˆ°çš„é¡µé¢æºç ä¸itemå¯¹è±¡ä¸€èµ·ä¼ é€’ç»™ä¸‹ä¸€ä¸ªè§£æå‡½æ•°
                   yield scrapy.Request(url=new_url,callback=self.content_parse,meta={'item': item})
           def content_parse(self,response):
               # æŠ½å–ä¼ é€’è¿‡æ¥çš„itemå¯¹è±¡
               item = response.meta['item']
               # è§£ææºç ä¸­çš„å†…å®¹æ•°æ®
               content = response.xpath('//*[@id="endText"]/p/text()').extract()
               # å°†å†…å®¹å­˜å‚¨è¿›itemå¯¹è±¡
               item['content'] = ''.join(content)
               # å°†æ•°æ®æäº¤ç»™ç®¡é“
               yield item
       
           def closed(self,spider):
               # å½“çˆ¬è™«ç»“æŸæ—¶,å…³é—­seleniumæµè§ˆå™¨
               self.pro.quit()
       ```

       ä»¥ä¸Šä¸ºçˆ¬è™«æ–‡ä»¶ä¸­çš„æ„é€ 

   2. ```python
       class WangyiPipeline(object):
           # åœ¨çˆ¬è™«å¼€å§‹è¿è¡Œæ—¶è¿›è¡Œæ–‡ä»¶çš„åˆ›å»º
           def open_spider(self, spider):
               self.fp = open('./news.txt', 'w', encoding='utf-8')
       
       	# å¯¹æäº¤çš„itemæ•°æ®è¿›è¡Œå¤„ç†
           def process_item(self, item, spider):
               title = item['title']
               content = item['content']
               # print(item)
               # å°†itemæ•°æ®è¿›è¡Œå­˜å‚¨
               self.fp.write('[' + 'title' + ':' + title + ',' + 'content' + ':' + content + ']' + '\n')
               return item
           def close_spider(self, spider):
               # çˆ¬è™«ç»“æŸæ—¶,å…³é—­æ‰“å¼€çš„æ–‡ä»¶
               self.fp.close()
       ```

       ä»¥ä¸Šä¸ºç®¡é“ä¸­çš„ä»£ç 

   3. ```python
       from scrapy import signals
       from scrapy.http import HtmlResponse
       from time import sleep
       class WangyiDownloaderMiddleware(object):
           # æ•è·å“åº”æ•°æ®å¹¶è¿›è¡Œæ£€æµ‹
           def process_response(self, request, response, spider):
               # æŠ½å–çˆ¬è™«æ–‡ä»¶ä¸­çš„æ¨¡æ‹Ÿæµè§ˆå™¨
               pro = spider.pro
               # å¯¹è¯·æ±‚å¯¹è±¡è¿›è¡Œè§£æ,å½“å¯¹è±¡ä¸ºä¸»é¡µé¢çš„æ—¶å€™,æ— è§†
               # spiderå‚æ•°ä¸ºå®ä¾‹åŒ–çš„çˆ¬è™«ç±»
               # å¯ä»¥ç›´æ¥è°ƒç”¨çˆ¬è™«ç±»çš„ä¸€äº›æ–¹æ³•
               if request.url in spider.cls_url_list:
                   # é€šè¿‡æ¨¡æ‹Ÿæµè§ˆå™¨å‘èµ·è¯·æ±‚,ç»•å¼€åŠ¨æ€åŠ è½½
                   pro.get(request.url)
                   # ç­‰å¾…ä¸€ç§’,ç¡®ä¿æ•°æ®åŠ è½½å®Œæˆ
                   sleep(1)
                   # è·å–é¡µé¢æºç 
                   page_text = pro.page_source
                   # å®ä¾‹åŒ–HtmlResponseå¯¹è±¡
                   # url    è¯·æ±‚çš„urlå¯¹è±¡
                   # body   è¦è¿”å›çš„é¡µé¢æºç æ•°æ®
                   # encoding  è®¾å®šé¡µé¢æºç çš„ç¼–ç æ ¼å¼
                   # request   è¯·æ±‚ä¸å˜
                   new_response = HtmlResponse(url=request.url,body=page_text,encoding='utf-8',request=request)
                   # å°†å‡†å¤‡å¥½çš„æ•°æ®è¿”å›
                   return new_response
               return response
       
       ```

       ä»¥ä¸Šä¸ºä¸­é—´ä»¶ä¸­çš„æ•°æ®

   4. åœ¨è¿™ä¸‰ç§æ¨¡å—çš„è¾…åŠ©ä¹‹ä¸‹,å¯ä»¥å®ç°å¯¹ç½‘ç«™æ•°æ®çš„æ·±åº¦çˆ¬å–,å¿…è¦æ—¶,å¯ä»¥æ·»åŠ é¡µç æ•°æ®,ä»è€Œç¡®ä¿è¿™ä¸ªçˆ¬è™«å¯ä»¥ç²¾ç¡®å®šä½åˆ°æ¯ä¸€ä¸ªåˆ†ç±»ä¸‹çš„æ‰€æœ‰æ•°æ®çš„æ¯ä¸€é¡µæ•°æ®,ä¹Ÿå°±æ˜¯è¯´,ä½¿ç”¨scrapyçˆ¬è™«,å¯ä»¥ç›´æ¥å®ç°å¯¹æ•´ä¸ªç›®æ ‡ç½‘ç«™çš„è¦†ç›–æ€§çˆ¬å–æ•°æ®

   5. åŒæ—¶,ç”±äºscrapyé»˜è®¤æ˜¯å¼‚æ­¥è¿è¡Œçš„,è¿™ç§å½¢æ€çš„çˆ¬è™«,å·¥ä½œæ•ˆç‡æ¯”èµ·requestsè¦é«˜å¾—å¤š

3. ### UserAgentä¼ªè£…

   1. ```python
      from fake_useragent import UserAgent
      a = UserAgent()
      print(a.Chrome)
      ```



